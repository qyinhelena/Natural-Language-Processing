{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_6",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5aFAKONGBpx"
      },
      "source": [
        "# **SETUP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITgKc9QbFwhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8818de0-b769-4d6f-b6f2-964008655479"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr1gKbt5FGll"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "#Sets random seeds for reproducibility\n",
        "seed=0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXwjsi_1Gckr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc9f6de-e303-4d8b-d6e2-2af22fd972ae"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n",
            "4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJzvqx-fUyDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0334b4-8699-4d6a-ca03-05c3e052c1a4"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW6/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW6/dev.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-04 21:34:13--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW6/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24595234 (23M) [text/plain]\n",
            "Saving to: ‘train.txt.1’\n",
            "\n",
            "train.txt.1         100%[===================>]  23.46M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-04 21:34:14 (191 MB/s) - ‘train.txt.1’ saved [24595234/24595234]\n",
            "\n",
            "--2021-04-04 21:34:14--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW6/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21540490 (21M) [text/plain]\n",
            "Saving to: ‘dev.txt.1’\n",
            "\n",
            "dev.txt.1           100%[===================>]  20.54M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-04-04 21:34:14 (230 MB/s) - ‘dev.txt.1’ saved [21540490/21540490]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7YdnBp5GgRV"
      },
      "source": [
        "# **IMPORTANT**: GPU is not enabled by default\n",
        "\n",
        "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yelj2vEDGjDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab9be05-5ed7-40a6-f159-f06b3198355b"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860qYZU9yidO"
      },
      "source": [
        "# Tip: Indexing into `torch.Tensor`\n",
        "\n",
        "In this section, we will briefly guide you through some examples of indexing into a 2D and 3D tensor that will be useful for the homework that follows.\n",
        "\n",
        "\n",
        "A [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) object is a multi-dimensional matrix that can be indexed in more than 2 dimensions. For example, you can create a 3D  tensor (of size 2 x 2 x 2) like this:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWbdF54UjkRV",
        "outputId": "a5e43b23-0ac9-49ee-b40a-7c8a8a2c3794"
      },
      "source": [
        "\n",
        "T_data = [[[1., 2.], [3., 4.]],\n",
        "          [[5., 6.], [7., 8.]]]\n",
        "T = torch.tensor(T_data)\n",
        "print(T.size())\n",
        "print(T)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 2])\n",
            "tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLWgNBq3j2Dd"
      },
      "source": [
        "You can index into this tensor and get a 2D matrix: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw3kMHTDjzC4",
        "outputId": "51b3102a-b774-4731-f025-230d40955d1f"
      },
      "source": [
        "print(T[0].size())\n",
        "print(T[0]) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNWQ9372kdZC"
      },
      "source": [
        "Here's an example of a 4 x 3 matrix (2D tensor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwc87fYZyiKJ",
        "outputId": "0d50dc21-348f-45c6-c391-f982f3437be3"
      },
      "source": [
        " mat = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
        " print(mat.size())\n",
        " mat"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3],\n",
              "        [ 4,  5,  6],\n",
              "        [ 7,  8,  9],\n",
              "        [10, 11, 12]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u3sWEvvkjxJ"
      },
      "source": [
        "Here are a number of different ways that you can index into that tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BbA1_JwkH7w",
        "outputId": "35cb4735-167c-4f73-e20f-b255765febbe"
      },
      "source": [
        "#Case1: Select 1,5,9,10\n",
        "print(\"Case1: \" + str(mat[(0,1,2,3),(0,1,2,0)])) #() or [] both work\n",
        "\n",
        "#Case2: Select first row\n",
        "print(\"Case2: \" + str(mat[0,]))\n",
        "#also mat[0,]\n",
        "#also mat[0,:]\n",
        "\n",
        "#Case3: Select all entries of the second column\n",
        "print(\"Case3: \" + str(mat[:,1]))\n",
        "#also mat[torch.arange((mat.size(0))), 1]\n",
        "#also mat[(0,1,2,3), 1]\n",
        "\n",
        "#Case4: Select the first three rows of the third column\n",
        "print(\"Case4: \" + str(mat[(0,1,2), 2]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Case1: tensor([ 1,  5,  9, 10])\n",
            "Case2: tensor([1, 2, 3])\n",
            "Case3: tensor([ 2,  5,  8, 11])\n",
            "Case4: tensor([3, 6, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEbxv8Qrgo02"
      },
      "source": [
        "Now (most relevant to the homework), let's say we have a three-dimensional tensor (e.g., batch size x number of words in the sentence x 768 (the BERT dimension); each row in the first dimension is a sentence, the second dimension corresponds to a WordPiece token within a sentence, and the third dimension the BERT embedding.  Let's say that we want to index into different words for each sentence (for example, the predicate might be at WordPiece position #3 in the first sentence and position #1 in the second sentence). What we want to end up with is a 2 x 3 selection from that matrix (just pulling out those respective vectors that correspond to the BERT embeddings for the predicate position). Here's how we can do that for a sample tensor of size 2 x 4 x 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmC7zrqJfPeX",
        "outputId": "eba914c9-e314-4b9b-d2a8-5bc0aac7cd2b"
      },
      "source": [
        " mat = torch.tensor([ \n",
        "                     [\n",
        "                      [1,2,3],[4,5,6],[7,8,9],[10,11,12] \n",
        "                     ],\n",
        "                     [\n",
        "                      [13,14,15],[16,17,18],[19,20,21],[22,23,24]\n",
        "                     ]\n",
        "                     ])\n",
        " \n",
        "print(mat.size())\n",
        "print(mat)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 3])\n",
            "tensor([[[ 1,  2,  3],\n",
            "         [ 4,  5,  6],\n",
            "         [ 7,  8,  9],\n",
            "         [10, 11, 12]],\n",
            "\n",
            "        [[13, 14, 15],\n",
            "         [16, 17, 18],\n",
            "         [19, 20, 21],\n",
            "         [22, 23, 24]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlBnZetFgB6D",
        "outputId": "fd42355e-0af1-4085-c449-9fb4a33e7da9"
      },
      "source": [
        "# e.g., the predicate WP index is #3 in the first sentence in the batch, and #1 in the second sentence\n",
        "indexes=[3,1]\n",
        "selected=mat[torch.arange(mat.size(0)),indexes]\n",
        "print(selected)\n",
        "print(selected.size())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[10, 11, 12],\n",
            "        [16, 17, 18]])\n",
            "torch.Size([2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onz1Ur1BrNHj"
      },
      "source": [
        "# Deliverable 1: Predict the semantic role of arguments for prose-argument pairs in a sentence. \n",
        "\n",
        "In this section, we will train a BERT-based classifier to assign proto-role labels (`ARG0`, `ARG1`, or `O`) to arguments in a given predicate-argument pair. \n",
        "\n",
        "The `BERTSRLClassifier` class is provided for you below, along with code to read in the data and train the model. This BERT-based classifier takes in the words of a sentence along with information about the beginning and end of an argument span and predicate to classify these predicate-specific arguments into `ARG0`, `ARG1`, or `O`(neither). \n",
        "\n",
        "See the writeup for a full description of the parts of the model you should implement.  To summarize, the `forward` function in the `BERTSRLClassifier` class concatenates the 786-dimensional BERT vectors that are indexed by:\n",
        "1. the start WordPiece token position of the argument span,\n",
        "2. the end WordPiece token position of the argument span,\n",
        "3. and the start WordPiece token position of the predicate\n",
        "\n",
        "and passes them through a linear transformation into the size of the 3-dimensional output space (for the three labels `ARG0`, `ARG1`, `O`).  Your deliverable is to complete the indicated section of the `forward` function in the `BERTSRLClassifier` class, where you will extract the BERT vectors corresponding to the positions (described above) from the final BERT layer before concatenating them and passing them through the linear transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZotzM7cE2ft"
      },
      "source": [
        "max_toks=56\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False, do_basic_tokenize=False)\n",
        "\n",
        "labels={\"ARG0\":0, \"ARG1\":1, \"O\":2}\n",
        "\n",
        "\n",
        "#return start and end positions of WordPiece tokens\n",
        "def get_wp_position_for_token(words, index):\n",
        "\n",
        "\tcur=1\n",
        "\ttargetWP=None\n",
        "\n",
        "\tfor idx, word in enumerate(words):\n",
        "\t\ttarget=tokenizer.tokenize(word)\n",
        "\t\tif idx == index:\n",
        "\t\t\ttargetWP=target\n",
        "\t\t\treturn cur, cur+len(target)\n",
        "\t\tcur+=len(target)\n",
        "\t\n",
        "\n",
        "def read_data(filename, max_toks=max_toks):\n",
        "\n",
        "\tx=[]\n",
        "\ty=[]\n",
        "\tm=[]\n",
        "\n",
        "\tmax_num=5000\n",
        "\tidx=0\n",
        "\n",
        "\twith open(filename) as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tcols=line.rstrip().split(\"\\t\")\n",
        "\t\t\twords=cols[0].split(\" \")\n",
        "\t\t\tpredicate=int(cols[1])\n",
        "\t\t\targuments=json.loads(cols[2])\n",
        "\t\t\tcandidates=json.loads(cols[3])\n",
        "\t\t\t\n",
        "\t\t\tnonargs=[]\n",
        "\n",
        "\t\t\tfor cat, start, end in arguments:\n",
        "\t\t\t\tif cat == \"ARG0\" or cat == \"ARG1\":\n",
        "\t\t\t\t\tx.append(words)\n",
        "\t\t\t\t\ty.append(labels[cat])\n",
        "          # store position of WordPiece tokens for start of span, end of span, and start of predicate\n",
        "\t\t\t\t\tm.append((get_wp_position_for_token(words, start)[0], get_wp_position_for_token(words, end)[1], get_wp_position_for_token(words, predicate)[0]))\n",
        "\n",
        "\t\t\t\t\tidx+=1\n",
        "\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tnonargs.append((cat, start, end))\n",
        "\n",
        "\n",
        "\t\t\t# select random non-ARG0 or ARG1\n",
        "\t\t\tcat, start, end=random.choice(nonargs)\n",
        "\t\t\tx.append(words)\n",
        "\t\t\ty.append(labels[\"O\"])\n",
        "      # store position of WordPiece tokens for start of span, end of span, and start of predicate) \n",
        "\t\t\tm.append((get_wp_position_for_token(words, start)[0], get_wp_position_for_token(words, end)[1], get_wp_position_for_token(words, predicate)[0]))\n",
        "\n",
        "\t\t\tidx+=1\n",
        "\n",
        "\t\t\tif idx >= max_num:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\treturn x, y, m"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikU--QcmFF2_"
      },
      "source": [
        "class BERTSRLClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "            \n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "        self.num_labels = 3    \n",
        "        self.fc = nn.Linear(3*768, self.num_labels)\n",
        "\n",
        "    def get_batches(self, all_x, all_y, all_m, batch_size=32, max_toks=max_toks):\n",
        "        \"\"\"\n",
        "        Get batches for input x, y, and m, with data tokenized according to the\n",
        "        #BERT tokenizer (and limited to a maximum number of WordPiece tokens)\n",
        "        \"\"\"\n",
        "\n",
        "        batches_x=[]\n",
        "        batches_y=[]\n",
        "        batches_m=[]\n",
        "\n",
        "        #The input sentence starts with a [CLS] tag and is followed by a [SEP] tag\n",
        "        for i in range(0, len(all_x), batch_size):\n",
        "\n",
        "            current_batch=[]\n",
        "\n",
        "            xb=all_x[i:i+batch_size]\n",
        "\n",
        "            current_batch_input_ids=[]\n",
        "            current_batch_attention_mask=[]\n",
        "\n",
        "            for s, sent in enumerate(xb):\n",
        "\n",
        "                sent_wp_tokens=[self.tokenizer.convert_tokens_to_ids(\"[CLS]\")]\n",
        "                attention_mask=[1]\n",
        "\n",
        "                for word in sent:\n",
        "                    toks = self.tokenizer.tokenize(word)\n",
        "                    toks = self.tokenizer.convert_tokens_to_ids(toks)\n",
        "                    sent_wp_tokens.extend(toks)\n",
        "                    attention_mask.extend([1]*len(toks))\n",
        "                \n",
        "                sent_wp_tokens.append(self.tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
        "                attention_mask.append(1)\n",
        "\n",
        "                current_batch_input_ids.append(sent_wp_tokens)\n",
        "                current_batch_attention_mask.append(attention_mask)\n",
        "\n",
        "            max_len = max([len(s) for s in current_batch_input_ids])\n",
        "\n",
        "            for j, sent in enumerate(current_batch_input_ids):\n",
        "                for k in range(len(current_batch_input_ids[j]), max_len):\n",
        "                    current_batch_input_ids[j].append(0)\n",
        "                    current_batch_attention_mask[j].append(0)\n",
        "\n",
        "            batch_y=all_y[i:i+batch_size]\n",
        "            batch_m=all_m[i:i+batch_size]\n",
        "\n",
        "            batches_x.append((torch.LongTensor(current_batch_input_ids).to(device), torch.LongTensor(current_batch_attention_mask).to(device)))\n",
        "            batches_y.append(torch.LongTensor(batch_y).to(device))\n",
        "            batches_m.append(torch.LongTensor(batch_m).to(device))\n",
        "                \n",
        "        return batches_x, batches_y, batches_m\n",
        "      \n",
        "\t\n",
        "\t\n",
        "\n",
        "    def forward(self, batch_x, batch_m): \n",
        "\n",
        "            bert_output = self.bert(input_ids=batch_x[0],\n",
        "                                                attention_mask=batch_x[1],\n",
        "                                                output_hidden_states=True, return_dict=True)\n",
        "\n",
        "            bert_hidden_states = bert_output['hidden_states']\n",
        "\n",
        "            out = bert_hidden_states[-1] # last BERT layer\n",
        "\n",
        "            start_span_indexes=batch_m[:,0]\n",
        "            end_span_wp_indexes=batch_m[:,1]\n",
        "            predicate_wp_indexes=batch_m[:,2]\n",
        "\n",
        "            '''\n",
        "            Extract the representation of the WP token in the start and end WP position of each argument \n",
        "            and the start WP position of the predicate from the last layer output\n",
        "\n",
        "            Then, concatenate the vectors and pass them through a linear transformation\n",
        "            '''\n",
        "            # YOUR CODE STARTS HERE\n",
        "            start_span = out[torch.arange(out.size(0)), start_span_indexes]\n",
        "            end_span = out[torch.arange(out.size(0)), end_span_wp_indexes]\n",
        "            start_predicate = out[torch.arange(out.size(0)), predicate_wp_indexes]\n",
        "            \n",
        "            # concatenate three vectors\n",
        "            cat = torch.cat((start_span, end_span, start_predicate), 1) # dim 3 * 768\n",
        "\n",
        "            # pass to linear transformation\n",
        "            out = self.fc(cat)\n",
        "\n",
        "            ## YOUR CODE ENDS HERE\n",
        "\n",
        "            return out.squeeze()\n",
        "\n",
        "    def evaluate(self, batch_x, batch_y, batch_m):\n",
        "\t\t\t\n",
        "        self.eval()\n",
        "        corr = 0.\n",
        "        total = 0.\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "                for x, y, m in zip(batch_x, batch_y, batch_m):\n",
        "                    y_preds = self.forward(x, m)\n",
        "                    for idx, y_pred in enumerate(y_preds):\n",
        "                        prediction=torch.argmax(y_pred)\n",
        "                        if prediction == y[idx]:\n",
        "                            corr += 1.\n",
        "                        total+=1                          \n",
        "        return corr/total\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIMml1JypBpr"
      },
      "source": [
        "classifier=BERTSRLClassifier()\n",
        "classifier.to(device)\n",
        "\n",
        "trainData='train.txt'\n",
        "devData='dev.txt'\n",
        "\n",
        "x,y,m=read_data(trainData)\n",
        "train_x, train_y, train_m=classifier.get_batches(x,y,m)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psDHgFfV_JYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f197813e-0850-4017-b9b0-2ee129df0c8f"
      },
      "source": [
        "x,y,m=read_data(devData)\n",
        "dev_x, dev_y, dev_m=classifier.get_batches(x,y,m)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-5)\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=5\n",
        "\n",
        "#accuracy before training\n",
        "accuracy=classifier.evaluate(dev_x, dev_y, dev_m)\n",
        "print(\"Accuracy: %.3f\" % accuracy)\n",
        "\t\n",
        "for epoch in range(num_epochs):\n",
        "\tclassifier.train()\n",
        "\n",
        "\t# Train\n",
        "\tfor x, y, m in zip(train_x, train_y, train_m):\n",
        "\t\ty_pred = classifier.forward(x, m)\n",
        "\t\tloss = cross_entropy(y_pred.view(-1, classifier.num_labels), y.view(-1))\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t# Evaluate\n",
        "\taccuracy=classifier.evaluate(dev_x, dev_y, dev_m)\n",
        "\tprint(\"Accuracy: %.3f\" % accuracy)\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.268\n",
            "Accuracy: 0.861\n",
            "Accuracy: 0.900\n",
            "Accuracy: 0.907\n",
            "Accuracy: 0.909\n",
            "Accuracy: 0.915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpMLEiCz8s2"
      },
      "source": [
        "# Deliverable 2: Find `ARG0` from a list of all non-terminal phrases in a parse tree \n",
        "\n",
        "Now, you will be given a sentence with all of its non-terminal phrases (candidates), where each candidate is given by `[NP category, start token position, end token position]`. For simplicity, the number of candidates for the given sentence doesn't exceed the system's batch size (32).\n",
        "\n",
        "\n",
        "In this section, identify the most likely `ARG0` among all the candidates, using our trained classifier, `BERTSRLClassifer`. In order to do this, you would generate an `ARG0` score for all the candidates using our classifier and select the candidate with the highest score (highest likelihood of being `ARG0`) as `ARG0`. The basic setup has been provided for you, using functions from the previous section**\\***. Your task is to finish the `predict_arg0` function so that it returns the **start and end position** of the `ARG0` predicted by your model. For example, if the sentence is \"I ate the cake\" (as given in the HW writeup), your function should return [0,0]. \n",
        "\n",
        "\\* *Note that we don't have y-labels for the list of candidates for this test case. In the provided code, we create a list of labels `test_y` and arbitrarily fill it with `O` because we need this list to run `get_batches`. The value in this list doesn't matter, as you won't be needing the output(`t_y`) to evaluate your result.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chqqe1Zh4Q4d"
      },
      "source": [
        "#This is the sentence you will be working with\n",
        "sent = \"Also , can animals remember images on TV like us , humans ?\"\n",
        "verb = 4 #predicate position\n",
        "candidate_list = '''[\n",
        "                    [\"RB\", 0, 0], \n",
        "                    [\"ADVP\", 0, 0],\n",
        "                    [\",\", 1, 1],\n",
        "                    [\"MD\", 2, 2], \n",
        "                    [\"NNS\", 3, 3], \n",
        "                    [\"NP\", 3, 3], \n",
        "                    [\"VB\", 4, 4], \n",
        "                    [\"NNS\", 5, 5], \n",
        "                    [\"NP\", 5, 5], \n",
        "                    [\"IN\", 6, 6], \n",
        "                    [\"NN\", 7, 7], \n",
        "                    [\"NP\", 7, 7], \n",
        "                    [\"PP\", 6, 7], \n",
        "                    [\"NP\", 5, 7], \n",
        "                    [\"IN\", 8, 8], \n",
        "                    [\"PRP\", 9, 9], \n",
        "                    [\"NP\", 9, 9], \n",
        "                    [\",\", 10, 10], \n",
        "                    [\"NNS\", 11, 11],\n",
        "                    [\"NP\", 11, 11],\n",
        "                    [\"NP\", 9, 11], \n",
        "                    [\"PP\", 8, 11],\n",
        "                    [\"VP\", 4, 11],\n",
        "                    [\".\", 12, 12], \n",
        "                    [\"SQ\", 0, 12], \n",
        "                    [\"TOP\", 0, 12]\n",
        "                    ] '''"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq69QeAd-S-A"
      },
      "source": [
        "#read in the sentence information\n",
        "#we don't have labels (y) for this task, but will create an arbitrary list in order to run get_batches()\n",
        "test_x = []\n",
        "test_m = []\n",
        "test_y = [] \n",
        "\n",
        "words = sent.split(\" \")\n",
        "predicate = int(verb)\n",
        "candidates = json.loads(candidate_list)\n",
        "\n",
        "\n",
        "for synt, start, end in candidates: \n",
        "  test_x.append(words)\n",
        "  test_y.append(labels[\"O\"]) #fill in with \"O\" (can be ARG0 or ARG1, doesn't matter)\n",
        "  #store position of WordPiece tokens for start of span, end of span, and start of predicate) \n",
        "  test_m.append((get_wp_position_for_token(words, start)[0], get_wp_position_for_token(words, end)[1], get_wp_position_for_token(words, predicate)[0]))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F88nyQSdClRg"
      },
      "source": [
        "t_x, t_y, t_m = classifier.get_batches(test_x,test_y,test_m) "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwGV1Nw6FZVG"
      },
      "source": [
        "def predict_arg0(batch_x, batch_m, cand_list):\n",
        "    \"\"\"\n",
        "    This function returns the start and end position of the predicted ARG0\n",
        "    \"\"\"\n",
        "    with torch.no_grad():    \n",
        "    #YOUR CODE STARTS HERE\n",
        "      candidates = [row[1:] for row in cand_list] # take last two columns\n",
        "      new_batch_m = torch.tensor(candidates, dtype=torch.int64).to(device) # turn into tensor\n",
        "      new_batch_m = [torch.cat((new_batch_m+1, (torch.ones(26, dtype=torch.int64)*(verb+1)).reshape(-1,1).to(device)), 1)] # add last column for predicate index\n",
        "      # print('batch_m\\n', batch_m)\n",
        "      # print('new_batch_m\\n', new_batch_m)\n",
        "      for x, m in zip(batch_x, new_batch_m):\n",
        "        # print('x:', x)\n",
        "        # print('m:', m)\n",
        "        y_preds = classifier.forward(x, m)\n",
        "        # print(y_preds)\n",
        "        # print(torch.argmax(y_preds, dim=0))\n",
        "        max_idx = int(torch.argmax(y_preds, dim=0)[0]) # get max ARG0 score\n",
        "        # print(max_idx)\n",
        "        # print(cand_list[max_idx])\n",
        "        positions = cand_list[max_idx][1:]\n",
        "\n",
        "    ##YOUR CODE ENDS HERE\n",
        "    \n",
        "    return positions"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JiQHOrT8XcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89284908-eef5-429a-ae79-d1dd53e86fbc"
      },
      "source": [
        "#Run this cell to print your prediction\n",
        "#The printed output should look like [a, b]\n",
        "predict_arg0(t_x, t_m, candidates)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}