{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp21/blob/master/HW2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4KuVSCSqlUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f66650-d30d-4e43-f07f-71830184e42f"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll use to make predictions for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn0XtfFeqP2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc15f5e4-2bff-4982-c37b-5a053adb71d0"
      },
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/test.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-01 03:35:37--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]   1.26M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-02-01 03:35:38 (14.7 MB/s) - ‘train.txt’ saved [1322055/1322055]\n",
            "\n",
            "--2021-02-01 03:35:38--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.25M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-02-01 03:35:38 (15.3 MB/s) - ‘dev.txt’ saved [1309909/1309909]\n",
            "\n",
            "--2021-02-01 03:35:38--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6573426 (6.3M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   6.27M  25.2MB/s    in 0.2s    \n",
            "\n",
            "2021-02-01 03:35:38 (25.2 MB/s) - ‘test.txt’ saved [6573426/6573426]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "                \n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "*First*, let's define a classifier based on a really simple dictionary-based feature: if the abstract contains the words \"love\" or \"like\", the CONTAINS_POSITIVE_WORD feature will fire, and if it contains either \"hate\" or \"dislike\", the CONTAINS_NEGATIVE_WORD will fire.  Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "            \n",
        "    return feats\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data.  Note the `L2_regularization_strength` specifies the strength of the L2 regularizer (values closer to 0 = stronger regularization), and the `min_feature_count` specifies how many data points need to contain a feature for it to be allowable as a feature in the model.  Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnqjxd6fKPiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a6d236-e6ac-4f4d-8177-15e3b8532dd0"
      },
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.509, Dev accuracy: 0.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "First, is this accuracy score any good?  Let's calculate the accuracy of a majority class predictor to provide some context.  Again, this determines the most represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t--LfOjPj7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad90822a-8d7e-4b09-d5a7-4fbc57b8a638"
      },
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "    \n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "            \n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "# Your assignment\n",
        "\n",
        "## Deliverable 1\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm three additional distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data. \n",
        "\n",
        "Describe your features and report their performance in the table below; implement the features in the specified `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance.  \n",
        "\n",
        "|Feature|Why should it work? (50 words each)|Dev set performance|\n",
        "|---|---|---|\n",
        "|Bag of words| Having a binary value for each word showed up in the review can us figure out the correlation between the word and the label pos/neg.|0.771\n",
        "|Feature 1| Counting the number of times that a typical sentiment phrase of two-grams correlated with label pos or label neg can help us inform the model to accurately predict the label. Also, words starting with un- and over- are usually linked with negative sentiments, so we include those words as well.|0.599\n",
        "|Feature 2| Counting the number of positive words and negative words shown in the AFINN dictionary can help the model predict positive or negative label.|0.703\n",
        "|Feature 3| Calculating the sum of positive sentiment scores, negative sentiment scores, and overall sentiment scores using the AFINN dictionary can help the model understand the direction and magnitude of the label for positive or negative.|0.713\n",
        "\n",
        "Note that it is not required for your features to actually perform well, but your justification for why it *should* perform better than a bag of words should be defensible.  The most creative features (defined as features that few other students use and that are reasonably well-performing) will receive extra credit for this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        \n",
        "        if word not in feats:\n",
        "          feats[word] = 1\n",
        "\n",
        "    return feats"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3AJ5qMBeqmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27252dc7-d915-4198-8562-f16b06c536bb"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: bag_of_words, Features: 21224, Train accuracy: 1.000, Dev accuracy: 0.771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocPMYhIt4BX0"
      },
      "source": [
        "'''feature 1: count of typical sentiment phrases or words'''\n",
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        if re.search('un.+', word.lower()) or re.search('over.+', word.lower()):\n",
        "          if word not in feats:\n",
        "            feats[word] = 1\n",
        "          else:\n",
        "            feats[word] += 1\n",
        "\n",
        "    for i in range(1, len(words)):\n",
        "        twoGram = str(words[i-1]).lower() + ' ' + str(words[i])\n",
        "\n",
        "        if twoGram in ['i like', 'i love', 'how amazing']:\n",
        "          feats['contains_pos'] = 1\n",
        "          if twoGram not in feats:\n",
        "            feats[twoGram] = 1\n",
        "          else:\n",
        "            feats[twoGram] += 1\n",
        "        if twoGram in [\"i don't\", \"not like\", \"don't like\", 'i hate', 'i dislike','fast forward',\n",
        "                       'not funny', 'not cool', 'not good', 'better not', 'could have']:\n",
        "          feats['contains_neg'] = 1\n",
        "          if twoGram not in feats:\n",
        "            feats[twoGram] = 1\n",
        "          else:\n",
        "            feats[twoGram] += 1\n",
        "        \n",
        "    return feats\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MAwRwbQ7lVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1c4f67-fcda-4e4a-f949-e977de63a44f"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: feature1, Features: 940, Train accuracy: 0.816, Dev accuracy: 0.599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNLzWONUGJNv",
        "outputId": "2a79d284-dd4b-4af7-e8ee-ae7dab86af5f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfqK4vcaFcYZ",
        "outputId": "6bb09f18-f876-4069-82fe-aa14b1fa901e"
      },
      "source": [
        "# download a dictionary of AFINN, 2477 words. Source: http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html\n",
        "# with key as word and value as scale from very negative -5 to very positive +5\n",
        "afinnlst = [line.strip().split('\\t') for line in open('/content/drive/MyDrive/Info 259 NLP/AFINN-111.txt')]\n",
        "afinn = dict()\n",
        "for k, v in afinnlst:\n",
        "  afinn[k] = int(v)\n",
        "len(afinn)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "source": [
        "'''feature 2: count of positive or negative words found in AFINN dictionary'''\n",
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    feats = {'pos_in_AFINN': 0, 'neg_in_AFINN': 0}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "      word = word.lower()\n",
        "      if word in afinn:\n",
        "        sentScore = afinn[word]\n",
        "        if sentScore > 0:\n",
        "          feats['pos_in_AFINN'] += 1\n",
        "        elif sentScore < 0:\n",
        "          feats['neg_in_AFINN'] += 1\n",
        "\n",
        "    return feats"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgpuykF67oWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421b2429-2644-4514-bd1a-b8e204792b02"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: feature2, Features: 2, Train accuracy: 0.713, Dev accuracy: 0.703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "source": [
        "'''feature 3: sum of overall sentiment scores, sum of positive scores, and sum of negative scores based on AFINN dictionary'''\n",
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    total, pos, neg = 0, 0, 0\n",
        "    for word in words:\n",
        "      word = word.lower()\n",
        "      if word in afinn:\n",
        "        total += afinn[word]\n",
        "        if afinn[word] > 0:\n",
        "          pos += afinn[word]\n",
        "        elif afinn[word] < 0:\n",
        "          neg += afinn[word]\n",
        "    \n",
        "    feats = {'sumScore': total, 'posScore': pos, 'negScore': neg}\n",
        "            \n",
        "    return feats"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_f--utb7q4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33632eb-29e6-4001-841f-dcefad35b756"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: feature3, Features: 3, Train accuracy: 0.724, Dev accuracy: 0.713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "  all_feats={}\n",
        "  for feature in [bag_of_words, feature1, feature2, feature3]:\n",
        "    all_feats.update(feature(text))\n",
        "  return all_feats"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-tRUFTIdAqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21734d4-0aab-4916-b3bb-21a623f49227"
      },
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=0.2, min_feature_count=50)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate .csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: combiner_function, Features: 439, Train accuracy: 0.926, Dev accuracy: 0.761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        " ## Deliverable 2\n",
        "\n",
        "This code will generate a file named `combiner_function_predictions.csv`; download this file (using e.g. the file manager on the left panel in Colab) and submit this to GradeScope along with your notebook; the 5 systems with the highest performance (revealed after the submission deadline) will receive extra credit for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lgyoJm09pqe"
      },
      "source": [
        "## Interrogating classifiers\n",
        "\n",
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance.  **Note that nothing below this line requires any work on your part; treat these as useful tools for understanding what works and what doesn't.**\n",
        "\n",
        "1. First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels).  What kinds of mistakes is it making?  (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well). \n",
        "\n",
        "**Answer: Looking at the train accuracy and dev accuracy, we can see that the model is still overfitting, altough I've applied L2 regularization and min_feature_count to mitigate overfitting.**\n",
        "\n",
        "**My model is making 124 mistakes in predicting negative label, and 115 mistakes in predicting positive labels. The mistakes in both classess are more or less balanced, but performance is a little better in predicting postive labels.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ulxd1TosIMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "69ae4ab7-9271-4573-a463-1fd463541183"
      },
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plot_confusion_matrix(classifier.log_reg, classifier.devX, classifier.devY, ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "2. Next, let's look at the features that are most defining for each of the classes (ranked by how strong their corresponding coefficient is).  Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data?\n",
        "\n",
        "**Answer: When looking at the most defining features for the positive class, we find many positive words such as top, high, very, excellent, definitely, etc. However, there are also words like us, thought, man, takes, gives, let that are very much neutral and common in reviews. The set of features for positive class in general tend to be positive on their own, and some of them if combined with other words can express very different sentiments (e.g. definitely waste of time) or simply describe the time when the reviewer watched the movie (e.g. late night).**\n",
        "\n",
        "**On the other hand, we find some negative sentiment words (e.g. worst, waste, poor, stupid, least) on top of the list. We also find somewhat positive words like funny in the list; it possibly conveys the negation of funny with combination of other words (e.g. nothing funny, far from funny). Also, there are many neutral words (e.g. acting, actors, script) in the absence of context; they have more defining power than they should have, which limits the ability to generalize to unseen datasets. It can be helpful to include more n-gram features, like \"not funny\", \"worth watching/not worth watching\" to help flip the labels for those mistaken ones.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAyGuXIi9pqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599cee41-d3f5-4bc7-f4c0-89523521df52"
      },
      "source": [
        "big_classifier.printWeights(n=25)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos\t0.750\ttop\n",
            "pos\t0.531\thigh\n",
            "pos\t0.528\tlate\n",
            "pos\t0.524\taway\n",
            "pos\t0.516\tvery\n",
            "pos\t0.515\tdifferent\n",
            "pos\t0.501\tnew\n",
            "pos\t0.439\texcellent\n",
            "pos\t0.424\tperformance\n",
            "pos\t0.419\tus\n",
            "pos\t0.408\tdefinitely\n",
            "pos\t0.399\tdead\n",
            "pos\t0.397\tback\n",
            "pos\t0.392\tthought\n",
            "pos\t0.390\tsaw\n",
            "pos\t0.389\tdown\n",
            "pos\t0.384\tmind\n",
            "pos\t0.375\tman\n",
            "pos\t0.374\tcomedy\n",
            "pos\t0.370\ttakes\n",
            "pos\t0.355\thim\n",
            "pos\t0.355\twant\n",
            "pos\t0.346\tgives\n",
            "pos\t0.341\tlet\n",
            "pos\t0.331\t'\n",
            "\n",
            "neg\t-0.925\tworst\n",
            "neg\t-0.838\tacting\n",
            "neg\t-0.762\t?\n",
            "neg\t-0.694\tfunny\n",
            "neg\t-0.694\twaste\n",
            "neg\t-0.652\tpoor\n",
            "neg\t-0.642\twatching\n",
            "neg\t-0.605\tleast\n",
            "neg\t-0.600\tactors\n",
            "neg\t-0.541\tawful\n",
            "neg\t-0.534\tfar\n",
            "neg\t-0.531\tboring\n",
            "neg\t-0.530\tscript\n",
            "neg\t-0.528\tperformances\n",
            "neg\t-0.491\twould\n",
            "neg\t-0.486\tinstead\n",
            "neg\t-0.465\tme\n",
            "neg\t-0.463\tmoney\n",
            "neg\t-0.456\tstupid\n",
            "neg\t-0.450\tsupposed\n",
            "neg\t-0.445\tseems\n",
            "neg\t-0.444\tidea\n",
            "neg\t-0.441\tplay\n",
            "neg\t-0.440\twomen\n",
            "neg\t-0.430\tnothing\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "3. Next, let's look at the individual data points that are most mistaken. Does it suggest any features you might create to disentangle them?\n",
        "\n",
        "**Answer: Among the top 20 most mistanken data points, 13 of them have ground true label of negative. After reading these most mistkened reviews, I find that these reviews are too nice (or implicit) in the way of expressing negative sentiments. Our model is not good at identifying these implicit meaning when a majority of features are looking at word-level. To disentangle them, I think it could be useful to include some common words (such as but, not) that are used in these implicit negative reviews. Further, I will try to add more features at the phrase level (n-gram). I will also take another look at the bag of words and see if there's way to reduce the dimensions that dominate the defining power over the relatively smaller number of n-gram features.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "source": [
        "def analyze(classifier):\n",
        "    \n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXmRhSuzxaJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5c787f1-59f4-4e7c-a89f-3aa3323e9842"
      },
      "source": [
        "analyze(big_classifier)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1551</td>\n",
              "      <td>0.999936</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I firmly believe that the best Oscar ceremony in recent years was in 2003 for two reasons: 1 ) Host Steve Martin was at his most wittiest: \" I saw the teamsters help Michael Moore into the trunk of his limo \" and \" I'll better not mention the gay mafia in case I wake up with a poodle's head in my bed \" 2 ) Surprise winners: No one had Adrien Brody down for best actor ( Genuine applause ) or Ro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1004</td>\n",
              "      <td>0.999120</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1436</td>\n",
              "      <td>0.997562</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1659</td>\n",
              "      <td>0.990810</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1913</td>\n",
              "      <td>0.986488</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I have only seen Gretchen Mol in two other films (Girl 6, Donnie Brasco), and don't really remember her, but she did a great job as a naive girl who posed for pictures because it made people happy.She really didn't think what she was doing was wrong, even when she left the business and found her religion again.The photos she made were certainly tame by today's standards, and it is funny seeing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1929</td>\n",
              "      <td>0.981014</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>At the name of Pinter, every knee shall bow - especially after his Nobel Literature Prize acceptance speech which did little more than regurgitate canned, by-the-numbers, sixth-form anti-Americanism. But this is even worse; not only is it a tour-de-force of talentlessness, a superb example of how to get away with coasting on your decades-old reputation, but it also represents the butchery of a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1309</td>\n",
              "      <td>0.978589</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1784</td>\n",
              "      <td>0.978583</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>The only complaint I have about this adaptation is that it is sexed-up. Things that were only hinted at in the novel are shown on-screen for some weird reason. Did they think the audience would be too stupid to understand if they were not shown everything out-right? Other than that, this is very good-quality. All the actors do marvelous jobs bringing their characters to life. For the shallow w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1206</td>\n",
              "      <td>0.972073</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Plot Synopsis: Hong Kong, 1966. Paul Wagner, the man who built the Victoria Tunnel, is murdered along with his wife by his associates. His twin sons, Chad &amp; Alex, are split apart. 25 years later, Chad, a karate instructor in Los Angeles, &amp; Alex, a smuggler living in Hong Kong, join forces to avenge their parents' murder &amp; rightfully claim the tunnel.This is the second time that Jean-Claude Van...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1464</td>\n",
              "      <td>0.971872</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I really wanted to like this film, but so much of it is stolen/borrowed from other work -- some of the borrowing is painfully blatant. The New York Times' review pointed out that their singing frog is awfully reminiscent of the one in the famous Warner Brothers' cartoon ('Hello my baby, hello my darlin', hello my ragtime gal...'). But I challenge anyone to watch the Fox/Blue Sky animated featu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1228</td>\n",
              "      <td>0.967670</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>This is quite a dull movie. Well-shot with realistic performances especially a very good one from Depardieu as a cad and bad boy with realistic locations mood and art-house connotations all over, it fails because the director takes no position, stand or critical commentary on the topic he stipulates. One of France's revered and regular working partner on films with Depardieu - I believe they m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1247</td>\n",
              "      <td>0.966907</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Let's start by the simple lines. From the viewer's side, there a couple of good \"director details\", some points of view at the movie scenes that are nice. The special effects are good enough, a good acting/good scenery also. But the story is way too simple. It shows how a elite Army bomb squad unit lives, acts and sometimes dies. It shows the drama of living in war. In my movie experience as a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1087</td>\n",
              "      <td>0.965757</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>That's what one of the girls said at the end.Is the soccer game a metaphor for a qualifying game between the girls (or more broadly, a free-thinking group) and the authority? \"To Germany\" means to a future that's of hope? It's one of the most unforgettable cinematic experience I've ever had -- despite the crude cinematography and plot, and mild over-acting (though I like the cast -- they're lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1833</td>\n",
              "      <td>0.962973</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>This was yet another big screen outing for a US TV show from the sixties It is amusing enough but was very much to formula. Intelligent Martian lands on Earth and meets the not too bright humans, in his view.The usual wackiness ensues with the human, Bridges, eventually bonds with him and helps him to get home. Along the way he also gets the girl, Hannah.This is a nice outing for some pleasant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1018</td>\n",
              "      <td>0.960475</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>This was surprisingly intelligent for a TV movie, and quite true to my own experience of bulimia. It was actually well-researched, and I can only assume it was written by someone who's gone through a similar experience, because it had all the little details. The characters were quite well-drawn, and the performances by Mare Winningham and Alison Lohman were great. I think what I like most was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1471</td>\n",
              "      <td>0.957974</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any charlie brown subtitler a run for his money he just constantly mumbles which is always a laugh, most scenes just feel awkward with characters more often than not gazing across to another with a look o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1533</td>\n",
              "      <td>0.955485</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I'm a big fan of Lucio Fulci; many of his Giallo and splatter flicks are amongst my favourites of all time, but this made for TV movie is extremely sub par and not what I've come to expect from the great Italian director. The film is neither interesting, like some of Fulci's more tame Giallo's, or gory like the majority of his cult classics; thus leaving it lacking in both major areas, and ult...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1847</td>\n",
              "      <td>0.954778</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I'm a great admirer of Lon Chaney, but the screen writing of this movie just did not work for me. The story jumps around oddly (I've since learned that the film is missing a section), and characters appear and disappear with irritating suddenness. Some of the intertitles are overly explanatory (e.g., \"why, you're not a child anymore!\"--cut back to picture for a long, slow beat--\"you're a woman...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1982</td>\n",
              "      <td>0.953396</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>The major flaw with the film is its uninspired script. It plods back and forth between vignettes of Bettie's story and re-creations of the Klaw short films. While the Klaw re-creations are well done, it is unnecessary to recreate them in their near entirety. Page Richards, while not an amazing actress, does a decent job overall. And, at times, she does bear a remarkable resemblance to Bettie. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1559</td>\n",
              "      <td>0.951721</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>A young scientist Harry Harrison is continuing his late father's scientific research into limb regeneration with flying colours, but his interferingly dominate mother and her doctor lover want to sell off the serum. When he finds out, there is an accident involving Harry losing an arm. So, he tries out the serum and what eventuates is a genetically deranged arm that has a mind of its own.Oh we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                                                                                                                                                                                                                                                                                                                                                                                                             Text\n",
              "0   1551  ...  I firmly believe that the best Oscar ceremony in recent years was in 2003 for two reasons: 1 ) Host Steve Martin was at his most wittiest: \" I saw the teamsters help Michael Moore into the trunk of his limo \" and \" I'll better not mention the gay mafia in case I wake up with a poodle's head in my bed \" 2 ) Surprise winners: No one had Adrien Brody down for best actor ( Genuine applause ) or Ro...\n",
              "1   1004  ...  Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...\n",
              "2   1436  ...  I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...\n",
              "3   1659  ...  Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...\n",
              "4   1913  ...  I have only seen Gretchen Mol in two other films (Girl 6, Donnie Brasco), and don't really remember her, but she did a great job as a naive girl who posed for pictures because it made people happy.She really didn't think what she was doing was wrong, even when she left the business and found her religion again.The photos she made were certainly tame by today's standards, and it is funny seeing...\n",
              "5   1929  ...  At the name of Pinter, every knee shall bow - especially after his Nobel Literature Prize acceptance speech which did little more than regurgitate canned, by-the-numbers, sixth-form anti-Americanism. But this is even worse; not only is it a tour-de-force of talentlessness, a superb example of how to get away with coasting on your decades-old reputation, but it also represents the butchery of a...\n",
              "6   1309  ...  Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...\n",
              "7   1784  ...  The only complaint I have about this adaptation is that it is sexed-up. Things that were only hinted at in the novel are shown on-screen for some weird reason. Did they think the audience would be too stupid to understand if they were not shown everything out-right? Other than that, this is very good-quality. All the actors do marvelous jobs bringing their characters to life. For the shallow w...\n",
              "8   1206  ...  Plot Synopsis: Hong Kong, 1966. Paul Wagner, the man who built the Victoria Tunnel, is murdered along with his wife by his associates. His twin sons, Chad & Alex, are split apart. 25 years later, Chad, a karate instructor in Los Angeles, & Alex, a smuggler living in Hong Kong, join forces to avenge their parents' murder & rightfully claim the tunnel.This is the second time that Jean-Claude Van...\n",
              "9   1464  ...  I really wanted to like this film, but so much of it is stolen/borrowed from other work -- some of the borrowing is painfully blatant. The New York Times' review pointed out that their singing frog is awfully reminiscent of the one in the famous Warner Brothers' cartoon ('Hello my baby, hello my darlin', hello my ragtime gal...'). But I challenge anyone to watch the Fox/Blue Sky animated featu...\n",
              "10  1228  ...  This is quite a dull movie. Well-shot with realistic performances especially a very good one from Depardieu as a cad and bad boy with realistic locations mood and art-house connotations all over, it fails because the director takes no position, stand or critical commentary on the topic he stipulates. One of France's revered and regular working partner on films with Depardieu - I believe they m...\n",
              "11  1247  ...  Let's start by the simple lines. From the viewer's side, there a couple of good \"director details\", some points of view at the movie scenes that are nice. The special effects are good enough, a good acting/good scenery also. But the story is way too simple. It shows how a elite Army bomb squad unit lives, acts and sometimes dies. It shows the drama of living in war. In my movie experience as a...\n",
              "12  1087  ...  That's what one of the girls said at the end.Is the soccer game a metaphor for a qualifying game between the girls (or more broadly, a free-thinking group) and the authority? \"To Germany\" means to a future that's of hope? It's one of the most unforgettable cinematic experience I've ever had -- despite the crude cinematography and plot, and mild over-acting (though I like the cast -- they're lo...\n",
              "13  1833  ...  This was yet another big screen outing for a US TV show from the sixties It is amusing enough but was very much to formula. Intelligent Martian lands on Earth and meets the not too bright humans, in his view.The usual wackiness ensues with the human, Bridges, eventually bonds with him and helps him to get home. Along the way he also gets the girl, Hannah.This is a nice outing for some pleasant...\n",
              "14  1018  ...  This was surprisingly intelligent for a TV movie, and quite true to my own experience of bulimia. It was actually well-researched, and I can only assume it was written by someone who's gone through a similar experience, because it had all the little details. The characters were quite well-drawn, and the performances by Mare Winningham and Alison Lohman were great. I think what I like most was ...\n",
              "15  1471  ...  But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any charlie brown subtitler a run for his money he just constantly mumbles which is always a laugh, most scenes just feel awkward with characters more often than not gazing across to another with a look o...\n",
              "16  1533  ...  I'm a big fan of Lucio Fulci; many of his Giallo and splatter flicks are amongst my favourites of all time, but this made for TV movie is extremely sub par and not what I've come to expect from the great Italian director. The film is neither interesting, like some of Fulci's more tame Giallo's, or gory like the majority of his cult classics; thus leaving it lacking in both major areas, and ult...\n",
              "17  1847  ...  I'm a great admirer of Lon Chaney, but the screen writing of this movie just did not work for me. The story jumps around oddly (I've since learned that the film is missing a section), and characters appear and disappear with irritating suddenness. Some of the intertitles are overly explanatory (e.g., \"why, you're not a child anymore!\"--cut back to picture for a long, slow beat--\"you're a woman...\n",
              "18  1982  ...  The major flaw with the film is its uninspired script. It plods back and forth between vignettes of Bettie's story and re-creations of the Klaw short films. While the Klaw re-creations are well done, it is unnecessary to recreate them in their near entirety. Page Richards, while not an amazing actress, does a decent job overall. And, at times, she does bear a remarkable resemblance to Bettie. ...\n",
              "19  1559  ...  A young scientist Harry Harrison is continuing his late father's scientific research into limb regeneration with flying colours, but his interferingly dominate mother and her doctor lover want to sell off the serum. When he finds out, there is an accident involving Harry losing an arm. So, he tries out the serum and what eventuates is a genetically deranged arm that has a mind of its own.Oh we...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxwwblfh9pqf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}