{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQNh_TKTxh5"
      },
      "source": [
        "import json\n",
        "import gensim\n",
        "import sys\n",
        "import gensim.downloader as api\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import nltk\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxDT4OEa6G_Q"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qDRQRrUzuaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6506f42f-df2f-4a50-d938-8ea4087a6b15"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8htiw0DFSFCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5046c5b2-fdc5-476c-cafa-74ce7ba03652"
      },
      "source": [
        "#Let's install qwikidata, which we will use to qurey the wikidata triples\n",
        "!pip install qwikidata\n",
        "#Let's install pywikibot, which we will use to query wikipedia articles\n",
        "!pip install pywikibot\n",
        "!export PYWIKIBOT_NO_USER_CONFIG=1\n",
        "!pip install wikipedia"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting qwikidata\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/40/4273aaaacd7269f80d8ce475aff7115ab8fce31488ba08f3eaca776d110a/qwikidata-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from qwikidata) (2.23.0)\n",
            "Collecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->qwikidata) (3.0.4)\n",
            "Installing collected packages: mypy-extensions, qwikidata\n",
            "Successfully installed mypy-extensions-0.4.3 qwikidata-0.4.0\n",
            "Collecting pywikibot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b5/589d4e6bc7b97f3be7997fdf79faab5a9aa25e4cf58fc64a11dc771252b0/pywikibot-6.0.1.tar.gz (493kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.1 in /usr/local/lib/python3.7/dist-packages (from pywikibot) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=20.2 in /usr/local/lib/python3.7/dist-packages (from pywikibot) (54.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.1->pywikibot) (2020.12.5)\n",
            "Building wheels for collected packages: pywikibot\n",
            "  Building wheel for pywikibot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pywikibot: filename=pywikibot-6.0.1-cp37-none-any.whl size=526659 sha256=adc2ee10f01fec32d80685b1231c4538ca518cc670a4fa698eccc90f74632abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f3/7d/918ba4b40f109ffffa23d3968a9a810cb5f1eac4e19f6ae7ef\n",
            "Successfully built pywikibot\n",
            "Installing collected packages: pywikibot\n",
            "Successfully installed pywikibot-6.0.1\n",
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11686 sha256=af55bb5e297531fda7736645cdd5fb67b8f882d959e24cf5e1273d431befa287\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF1Bh7YZMjVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44151631-5cbb-4092-e360-3b6d5e5bdfcd"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/glove.6B.50d.50K.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/dev_dataset.csv\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/train_dataset.csv\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 15:01:22--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/glove.6B.50d.50K.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21357789 (20M) [text/plain]\n",
            "Saving to: ‘glove.6B.50d.50K.txt’\n",
            "\n",
            "glove.6B.50d.50K.tx 100%[===================>]  20.37M   107MB/s    in 0.2s    \n",
            "\n",
            "2021-04-12 15:01:24 (107 MB/s) - ‘glove.6B.50d.50K.txt’ saved [21357789/21357789]\n",
            "\n",
            "--2021-04-12 15:01:24--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/dev_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86511 (84K) [text/plain]\n",
            "Saving to: ‘dev_dataset.csv’\n",
            "\n",
            "dev_dataset.csv     100%[===================>]  84.48K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2021-04-12 15:01:24 (12.1 MB/s) - ‘dev_dataset.csv’ saved [86511/86511]\n",
            "\n",
            "--2021-04-12 15:01:24--  https://raw.githubusercontent.com/dbamman/nlp21/main/HW7/train_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231438 (226K) [text/plain]\n",
            "Saving to: ‘train_dataset.csv’\n",
            "\n",
            "train_dataset.csv   100%[===================>] 226.01K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-04-12 15:01:24 (16.5 MB/s) - ‘train_dataset.csv’ saved [231438/231438]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C21-6PV4xqQf"
      },
      "source": [
        "user_config = '''\n",
        "mylang = 'en'\n",
        "family = 'wikipedia'\n",
        "username = 'ExampleBot'\n",
        "'''\n",
        "with open(\"./user-config.py\", \"w\") as f:\n",
        "  f.write(user_config)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNXmHiNw6jRl"
      },
      "source": [
        "# **IMPORTANT**: GPU is not enabled by default\n",
        "\n",
        "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bVEu6IP6j2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57aef3c-c654-40ab-9cc4-d0e08e637fd7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfvHlfmOLpf2"
      },
      "source": [
        "# Deliverable 1: Distant Supervision\n",
        "\n",
        "In this component, we will query wikipedia articles and structured triples of information.  We will align the triples with text via the algorithm in SLP Chapter 17 Figure 17.9.  We will be doing this for a small set of Wikipedia articles and relationship types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlyn7pSv5953"
      },
      "source": [
        "First, let's download the text of the Wikipedia articles we are interested in.  For the purpose of Deliverable 1, this is 50 Alternative Hip Hop artists.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cCco2We-OII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60349eee-2686-48ea-a417-2e578e42b18f"
      },
      "source": [
        "\n",
        "import pywikibot\n",
        "import wikipedia\n",
        "from pywikibot import pagegenerators\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import requests\n",
        "site = pywikibot.Site()\n",
        "\n",
        "#This line grabs all the wikipedia articles for a given category\n",
        "cat = pywikibot.Category(site,'Category:Alternative hip hop musicians')\n",
        "#Now, we will use pywikibot to grab all wikipedia pages linked to this category\n",
        "gen = pagegenerators.CategorizedPageGenerator(cat)\n",
        "\n",
        "\n",
        "documents = {}\n",
        "#For this homework, we will be stopping once 10 articles are queried\n",
        "counter = 0\n",
        "\n",
        "#iterates through all the pages\n",
        "for page in tqdm(gen):\n",
        "    #grabs the title as plaintext\n",
        "    title = page.title(with_ns=False)\n",
        "    \n",
        "    #If errors, we skip over the entity \n",
        "    try:\n",
        "      #grabs the wikipedia page text\n",
        "      text = wikipedia.page(title)\n",
        "      documents[title] = text.content\n",
        "      counter += 1 \n",
        "    except:\n",
        "      continue\n",
        "    if counter > 50:\n",
        "      break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pywikibot/config2.py:945: UserWarning: \n",
            "Configuration variable \"username\" is defined in your user-config.py\n",
            "but unknown. It can be a misspelled one or a variable that is no\n",
            "longer supported.\n",
            "  UserWarning)\n",
            "58it [00:48,  1.37it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzHkOspjd_sv"
      },
      "source": [
        "Let's look at an example of one of the returned wikipedia pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NyeGR9heCNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "7c62720d-6c58-4ab7-a6b6-c6ca21866831"
      },
      "source": [
        "documents['WebsterX']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'WebsterX is a Milwaukee-based alt-rap singer and songwriter, blending indie-rock and hip-hop. Motifs within his work contrast darkness and depression with hopeful upbeat and psychedelic instrumentation. The Fader speaks to his work as a \"personal dive into the rapper\\'s psyche\", as he wrestles with his \"doubts and fears\", but, ultimately provides \"glimmers of hope throughout\". WebsterX writes all his own lyrics and works with a number of producers to create the beats. In 2016 WebsterX was awarded Solo Artist of the Year by the Radio Milwaukee Music Awards, sponsored by Koss. Among his affiliates include Chance The Rapper, Bon Iver, SaveMoney crew and Mick Jenkins. In 2016, WebsterX signed to Chicago-based label Closed Sessions, who\\'s alumni include Vic Mensa, Kweku Collins, & Thelonius Martin. His song \"Intuition\" was featured in a Super Bowl ad for the Microsoft Surface, starring Arizona Cardinals wide receiver Larry Fitzgerald. WebsterX was set to perform alongside Bon Iver for his Wisconsin Get Out The Vote tour in October 2020 but has since been postponed due to COVID-19. His music has been featured in The Fader, NPR, Complex, PBS, Paper Magazine, Vice, Bonafide Magazine, Hypebeast, Koss, TimeOut, & Pitchfork.\\n\\n\\n== Career ==\\nInspired by a family legacy in music, Sam Ahmed was born to Ethiopian immigrant parents – his father was the notable Hari musician, Abdi Guitar of known African group, Roha Band. in 2015, Ahmed dropped out of the University of Wisconsin-Milwaukee and deciding to pursue music and art full-time.He released his debut mixtape Desperate Youth in 2013, his 2015 single “Doomsday” and its subsequent video earning national press. In late 2015, he released a collaborative EP with NAN producer Q the Sun, called KidX. The same year he performed with Riff Raff, and Chanel West Coast at historic Milwaukee music venue, The Rave.  In March 2017 he released his 15-track debut album Daymares, produced by Simen Sez, which was awarded No. 1 on the Journal Sentinel\\'s Best Milwaukee Albums of 2017 List. WebsterX performed at SXSW in both 2016 and 2018. Notable 2019 performances include opening for Lil Yachty at Freakfest. In fall of 2019 WebsterX was selected by director Enrique \"Mag\" Rodriguez, for an offer to join the Backline Music generator, a 12-week grant accelerator program developed by venture capital firm Gener8tor, which provides coaching, mentoring, industry networking, and grants to cohorts of up to three musicians annually.Aside from music, WebsterX supports youth-oriented community engagement and is a co-founder of organization FreeSpace, founded in 2015 as a free, monthly, all-ages music showcase that invites Milwaukee\\'s young creatives to come together and collaborate. Hosts bring together guest youth artists mainly from Milwaukee\\'s North and West sides and build skills as experienced headliners perform alongside emerging artists. Interviews are also conducted with the artist in front of an audience. Freespace evolved into The New State, an upcoming permanent all-ages music venue, engineering studio, consignment store and performance space in the heart of Milwaukee. WebsterX was additionally part of the Milwaukee music collective, New Age Narcissism. The group – whose members include Lex Allen, Siren, Lorde Fred33, Bo Triplex, Jay Anderson, and Christopher Gilbert – each having their own solo careers, sporadically performed and created together. The group \"toured\" and collaborated with Milwaukee Public Schools, performing at elementary and middle schools while educating students on music entrepreneurship.In 2020 WebsterX hosted the 3,000 rider strong Black Is Beautiful benefit bike ride in honor of POC Mental health, proceeds going to local charities ConnectMilwaukee & Wisconsin Bike Fed.\\n\\n\\n== Discography ==\\nEverfeel (2018)\\nAin\\'t My Fault (2018)\\nNo End (2018)\\nFeels (2018)\\nIntuition (2018)\\nDaymares (2017)\\nLost Ones Freestyle (2016)\\nBlue Streak (2016)\\nEverything (2016)\\nKinfolk (2015)\\nLately (2015)\\nDoomsday (2015)\\nKidX (2015)\\nDesperate Youth (2013)\\n\\n\\n== References =='"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIpUptt_7wjM"
      },
      "source": [
        "Because the Wikidata corpus is incredibly large, we will use a series of sparql queries to get relevant triples for our corpus.  We will return all relevant triples for alternative hip hop artists and filter out later.\n",
        "\n",
        "Please see https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/Wikidata_Query_Help \n",
        "for more details on Wikidata sparql queries if interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejT-QCy4pjG"
      },
      "source": [
        "'''\n",
        "Now we will define the 5 queries we will be using for the distant supervision.\n",
        "\n",
        "We are interested in artists' date of birth, place of birth, school attended,\n",
        "start of musician career, and band name.\n",
        "\n",
        "Please see https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/Wikidata_Query_Help \n",
        "for more information on how to structure queries like this, if interested\n",
        "'''\n",
        "\n",
        "q1 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?dob \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P569 ?dob.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "\n",
        "'''\n",
        "\n",
        "q2 = '''\n",
        "\n",
        "SELECT DISTINCT ?human ?humanLabel ?pobLabel \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P19 ?pob.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q3 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?schoolLabel \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P69 ?school.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q4 = '''\n",
        "\n",
        "SELECT DISTINCT ?human ?humanLabel ?start \n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P2031 ?start.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "'''\n",
        "\n",
        "q5 = '''\n",
        "SELECT DISTINCT ?human ?humanLabel ?memberLabel\n",
        "WHERE\n",
        "{\n",
        "    VALUES ?professions {wd:Q177220 wd:Q639669}\n",
        "    ?human wdt:P31 wd:Q5 .\n",
        "    ?human wdt:P106 ?professions .\n",
        "    ?human wdt:P136 ?genre .\n",
        "    ?human wikibase:statements ?statementcount .\n",
        "    ?human wdt:P136 wd:Q438503 .  \n",
        "    ?human wdt:P463 ?member.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "\n",
        "'''\n",
        "queries = [q1, q2, q3, q4, q5]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCeP63glSFE4"
      },
      "source": [
        "'''\n",
        "Next, let's query wikidata for the triples we are interested in \n",
        "\n",
        "This cell makes a sparql query to wikidata to return triples of info.\n",
        "'''\n",
        "from qwikidata.sparql import (get_subclasses_of_item,\n",
        "                              return_sparql_query_results)\n",
        "from datetime import datetime\n",
        "\n",
        "triples = []\n",
        "query_names = [\"hasDateOfBirth\", \"hasPlaceOfBirth\", \"hasSchool\", \"hasYearStarted\", \"hasMembershipOf\"]\n",
        "count = 0\n",
        "for query in queries:\n",
        "  res = return_sparql_query_results(query)\n",
        "  #We want to save all the triples which are returned from the query\n",
        "  for item in res['results']['bindings']: \n",
        "    if count == 0:\n",
        "      dt = datetime.fromisoformat(item[res['head']['vars'][2]]['value'].split(\"T\")[0])\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], str(dt.strftime(\"%B\")) + \" \" + str(dt.day) + \", \" + str(dt.year)))\n",
        "    elif count == 3:\n",
        "      dt = datetime.fromisoformat(item[res['head']['vars'][2]]['value'].split(\"T\")[0])\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], str(dt.year)))\n",
        "    else:\n",
        "      triples.append((query_names[count], item[res['head']['vars'][1]]['value'], item[res['head']['vars'][2]]['value']))\n",
        "  count += 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu4nbvOG8ZD1"
      },
      "source": [
        "Let's examine the format of one of these returned triples, indicating that Kelis was born on August 21, 1979."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cBaIbmKABxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed4a609-4d1d-42ea-e7d3-8302403c432f"
      },
      "source": [
        "print(triples[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('hasDateOfBirth', 'Kelis', 'August 21, 1979')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHbIC9V988fB"
      },
      "source": [
        "Now, let's iterate through our Wikipedia articles and align factual triples to sentences from our Wikipedia articles.\n",
        "\n",
        "You will add code to follow the entity alignment algorithm described in Figure 17.9 in SLP3.  Do not worry about the training component of the algorithm; this is covered in Deliverable 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPik_6-_IR4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b3394c-14dc-458f-83e5-5155e56901b9"
      },
      "source": [
        "'''\n",
        "Now, we will iterate through and align the triples to sentences to create the dataset.\n",
        "\n",
        "This uses the algorithm from SLP3 Figure 17.9.\n",
        "'''\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_dataset(documents, triples, query_names):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  aligned_dataset = []\n",
        "\n",
        "  for document in tqdm(documents):\n",
        "    doc = nlp(documents[document])\n",
        "    sentences = list(doc.sents)\n",
        "    for sent in sentences:\n",
        "      sent = sent.text\n",
        "      for relation in query_names:\n",
        "        for triple in [t for t in triples if t[0] == relation]:\n",
        "          should_align = False\n",
        "          \n",
        "          #YOUR CODE HERE\n",
        "          if triple[1] in sent and triple[2] in sent:\n",
        "            should_align = True\n",
        "\n",
        "          if should_align:\n",
        "            #Let's mark the entities with a special prefix and join the multi-word ones with underscores as in SLP\n",
        "            formatted_sent = sent.replace(triple[1], \"_ent1_\" + \"_\".join(triple[1].split(\" \")))\n",
        "            formatted_sent = formatted_sent.replace(triple[2], \"_ent2_\" + \"_\".join(triple[2].split(\" \")))\n",
        "            aligned_dataset.append((formatted_sent, triple[0]))\n",
        "          \n",
        "  return aligned_dataset\n",
        "aligned_dataset = process_dataset(documents, triples, query_names)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/51 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/51 [00:00<00:08,  5.71it/s]\u001b[A\n",
            "  4%|▍         | 2/51 [00:00<00:11,  4.38it/s]\u001b[A\n",
            "  6%|▌         | 3/51 [00:01<00:15,  3.04it/s]\u001b[A\n",
            "  8%|▊         | 4/51 [00:01<00:14,  3.31it/s]\u001b[A\n",
            " 10%|▉         | 5/51 [00:01<00:11,  4.07it/s]\u001b[A\n",
            " 14%|█▎        | 7/51 [00:01<00:08,  5.01it/s]\u001b[A\n",
            " 18%|█▊        | 9/51 [00:01<00:08,  5.18it/s]\u001b[A\n",
            " 20%|█▉        | 10/51 [00:02<00:08,  4.87it/s]\u001b[A\n",
            " 24%|██▎       | 12/51 [00:03<00:10,  3.71it/s]\u001b[A\n",
            " 25%|██▌       | 13/51 [00:03<00:08,  4.47it/s]\u001b[A\n",
            " 27%|██▋       | 14/51 [00:03<00:08,  4.39it/s]\u001b[A\n",
            " 29%|██▉       | 15/51 [00:04<00:15,  2.37it/s]\u001b[A\n",
            " 33%|███▎      | 17/51 [00:04<00:10,  3.13it/s]\u001b[A\n",
            " 35%|███▌      | 18/51 [00:04<00:12,  2.62it/s]\u001b[A\n",
            " 37%|███▋      | 19/51 [00:05<00:11,  2.80it/s]\u001b[A\n",
            " 39%|███▉      | 20/51 [00:05<00:12,  2.55it/s]\u001b[A\n",
            " 41%|████      | 21/51 [00:05<00:09,  3.25it/s]\u001b[A\n",
            " 43%|████▎     | 22/51 [00:05<00:07,  3.98it/s]\u001b[A\n",
            " 45%|████▌     | 23/51 [00:06<00:08,  3.29it/s]\u001b[A\n",
            " 47%|████▋     | 24/51 [00:08<00:20,  1.29it/s]\u001b[A\n",
            " 49%|████▉     | 25/51 [00:09<00:22,  1.16it/s]\u001b[A\n",
            " 53%|█████▎    | 27/51 [00:09<00:15,  1.59it/s]\u001b[A\n",
            " 55%|█████▍    | 28/51 [00:09<00:12,  1.90it/s]\u001b[A\n",
            " 57%|█████▋    | 29/51 [00:09<00:08,  2.48it/s]\u001b[A\n",
            " 59%|█████▉    | 30/51 [00:10<00:06,  3.15it/s]\u001b[A\n",
            " 61%|██████    | 31/51 [00:10<00:05,  3.52it/s]\u001b[A\n",
            " 63%|██████▎   | 32/51 [00:10<00:05,  3.18it/s]\u001b[A\n",
            " 65%|██████▍   | 33/51 [00:11<00:09,  1.89it/s]\u001b[A\n",
            " 67%|██████▋   | 34/51 [00:12<00:08,  2.01it/s]\u001b[A\n",
            " 71%|███████   | 36/51 [00:12<00:05,  2.64it/s]\u001b[A\n",
            " 73%|███████▎  | 37/51 [00:12<00:04,  3.22it/s]\u001b[A\n",
            " 75%|███████▍  | 38/51 [00:12<00:03,  3.94it/s]\u001b[A\n",
            " 76%|███████▋  | 39/51 [00:12<00:02,  4.78it/s]\u001b[A\n",
            " 78%|███████▊  | 40/51 [00:12<00:02,  4.13it/s]\u001b[A\n",
            " 80%|████████  | 41/51 [00:13<00:02,  4.45it/s]\u001b[A\n",
            " 84%|████████▍ | 43/51 [00:13<00:01,  4.71it/s]\u001b[A\n",
            " 86%|████████▋ | 44/51 [00:13<00:01,  4.77it/s]\u001b[A\n",
            " 88%|████████▊ | 45/51 [00:13<00:01,  5.49it/s]\u001b[A\n",
            " 90%|█████████ | 46/51 [00:14<00:00,  5.28it/s]\u001b[A\n",
            " 92%|█████████▏| 47/51 [00:15<00:02,  1.45it/s]\u001b[A\n",
            " 94%|█████████▍| 48/51 [00:16<00:01,  1.56it/s]\u001b[A\n",
            " 98%|█████████▊| 50/51 [00:16<00:00,  2.14it/s]\u001b[A\n",
            "100%|██████████| 51/51 [00:16<00:00,  3.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJJV56cXAhnn"
      },
      "source": [
        "Now let's look at our newly-aligned dataset, containing a small number of aligned triples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upSGQoJdC5dk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e86dd6d-8e70-4626-d90e-95f1eafc62f9"
      },
      "source": [
        "aligned_dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_ent1_Arabesque, also known as Besque (born Stephen Kawaleet, _ent2_September_17,_1981), is a Juno nominated hip hop artist from Toronto, Ontario, Canada.',\n",
              "  'hasDateOfBirth'),\n",
              " ('_ent1_Arabesque, also known as Besque (born Stephen Kawaleet, September 17, 1981), is a Juno nominated hip hop artist from _ent2_Toronto, Ontario, Canada.',\n",
              "  'hasPlaceOfBirth'),\n",
              " ('Later in the year, he toured with _ent1_Aceyalone from _ent2_Los_Angeles.',\n",
              "  'hasPlaceOfBirth'),\n",
              " ('Daniel Dewan Sewell (born _ent2_March_16,_1981), known professionally as _ent1_Danny_Brown, is an American rapper.',\n",
              "  'hasDateOfBirth'),\n",
              " (\"The Grey Album also got the attention of _ent1_Damon_Albarn, who enlisted Danger Mouse to produce _ent2_Gorillaz' second studio album, Demon Days.\",\n",
              "  'hasMembershipOf'),\n",
              " ('Erik Francis Schrody (born _ent2_August_18,_1969), known by his stage name _ent1_Everlast, is an American musician, singer, rapper, and songwriter, known for his solo work and as the frontman for hip hop group House of Pain.',\n",
              "  'hasDateOfBirth'),\n",
              " ('Erik Francis Schrody (born August 18, 1969), known by his stage name _ent1_Everlast, is an American musician, singer, rapper, and songwriter, known for his solo work and as the frontman for hip hop group _ent2_House_of_Pain.',\n",
              "  'hasMembershipOf'),\n",
              " (\"Following the album's failure, _ent1_Everlast teamed up with fellow Taft High alums DJ Lethal and Danny Boy to form _ent2_House_of_Pain in Los Angeles, California.\",\n",
              "  'hasMembershipOf'),\n",
              " ('early 2006, _ent1_Everlast teamed up with his former House of Pain mates DJ Lethal and Danny Boy to join the hip-hop group _ent2_La_Coka_Nostra.',\n",
              "  'hasMembershipOf'),\n",
              " ('early 2006, _ent1_Everlast teamed up with his former _ent2_House_of_Pain mates DJ Lethal and Danny Boy to join the hip-hop group La Coka Nostra.',\n",
              "  'hasMembershipOf'),\n",
              " (\"On September 7, 2018, _ent1_Everlast's seventh studio album Whitey Ford's _ent2_House_of_Pain was released.\\n\\n\\n\",\n",
              "  'hasMembershipOf'),\n",
              " ('Scott Ramon Seguro Mescudi (born _ent2_January_30,_1984), better known by his stage name _ent1_Kid_Cudi ( KUDD-ee; often stylized as KiD CuDi), is an American rapper, singer, songwriter, record producer, actor, and record executive.',\n",
              "  'hasDateOfBirth'),\n",
              " ('In July _ent2_2008, _ent1_Kid_Cudi released his first mixtape, A Kid Named Cudi (executive produced by Plain Pat and Emile Haynie), in collaboration with New York street-wear brand 10.Deep as a free download.',\n",
              "  'hasYearStarted'),\n",
              " ('_ent1_Kid_Cudi was a prominent songwriter and featured artist on 808s & Heartbreak, with \"Paranoid\" and \"Heartless\" being released as singles, while \"Welcome to Heartbreak\" charted as an album cut and peaked at number 87 on the Pop 100._ent1_Kid_Cudi\\'s first television appearance was at the _ent2_2008 MTV Video Music Awards, alongside Travis Barker and DJ AM.',\n",
              "  'hasYearStarted'),\n",
              " ('_ent1_Kid_Cudi\\'s sound is what inspired and led Kanye West to create his cathartic 808s & Heartbreak (_ent2_2008), with West later stating that he and Cudi were \"the originators of the style, kinda like what Alexander McQueen is to fashion….',\n",
              "  'hasYearStarted')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8v9zzyAoTm"
      },
      "source": [
        "We've now successfully used distant supervision to align sentences from Wikipedia articles to information triples from Wikidata.  Note that the dataset is not perfect, as it is done without human annotation.  This  process scales up without additional human effort, at the cost of more compute time.  For Deliverable 2, we will be providing a dataset created using Distant Supervision, as the compute-time required to create a sizable dataset is large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv5G5vQAMEk9"
      },
      "source": [
        "# Deliverable 2: Relation Prediction Model\n",
        "\n",
        "Now that we have the process to create an aligned dataset, let's train a CNN-based model to predict a relationship from the text spans.  Note that we will be using a different, larger dataset than the one you created in Deliverable 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loYVNi56BlTg"
      },
      "source": [
        "train_dataset = \"./train_dataset.csv\"\n",
        "dev_dataset = \"./dev_dataset.csv\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qbFLoebIR6d"
      },
      "source": [
        "'''\n",
        "Let's create a dictionary of relation types to define the classification output space\n",
        "For this deliverable we have an additional category, no_relation_found, which can be\n",
        "applied to sentences which do not align with a triple.\n",
        "'''\n",
        "query_names = [\"hasDateOfBirth\", \"hasPlaceOfBirth\", \"hasSchool\", \"hasYearStarted\", \"hasMembershipOf\", \"no_relation_found\"]\n",
        "\n",
        "labels = {}\n",
        "count = 0 \n",
        "for query in query_names:\n",
        "  labels[query] = count\n",
        "  count += 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lf72jabPSzK",
        "outputId": "ffff29ab-0a57-4b29-c777-472a1dea859a"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hasDateOfBirth': 0, 'hasPlaceOfBirth': 1, 'hasSchool': 2, 'hasYearStarted': 3, 'hasMembershipOf': 4, 'no_relation_found': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tCtGJOMeMa"
      },
      "source": [
        "'''\n",
        "s1 and s2 define the position embeddings\n",
        "'''\n",
        "def get_batches(x, s1, s2, y, xType, batch_size=12):\n",
        "    batches_x=[]\n",
        "    batches_s1 = []\n",
        "    batches_s2 = []\n",
        "    batches_y=[]\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        #import pdb; pdb.set_trace()\n",
        "        batches_x.append(xType(x[i:i+batch_size]))\n",
        "        batches_s1.append(xType(s1[i:i+batch_size]))\n",
        "        batches_s2.append(xType(s2[i:i+batch_size]))\n",
        "        batches_y.append(torch.LongTensor(y[i:i+batch_size]))\n",
        "    \n",
        "    return batches_x,batches_s1, batches_s2, batches_y"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0QeNSfb3FdW"
      },
      "source": [
        "PAD_INDEX = 0             # reserved for padding words\n",
        "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
        "SEP_INDEX = 2\n",
        "\n",
        "MAX_DATA_LEN = 300\n",
        "\n",
        "data_lens = []\n",
        "\n",
        "def read_embeddings(filename, vocab_size=50000):\n",
        "  \"\"\"\n",
        "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
        "  \n",
        "  Arguments:\n",
        "  - filename:     path to file\n",
        "                  automatically infers correct embedding dimension from filename\n",
        "  - vocab_size:   maximum number of embeddings to load\n",
        "\n",
        "  Returns \n",
        "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
        "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
        "\n",
        "  vocab = {}\n",
        "\n",
        "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx, line in enumerate(file):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols = line.rstrip().split(\" \")\n",
        "      val = np.array(cols[1:])\n",
        "      word = cols[0]\n",
        "      embeddings[idx + 2] = val\n",
        "      vocab[word] = idx + 2\n",
        "  \n",
        "  # a FloatTensor is a multidimensional matrix\n",
        "  # that contains 32-bit floats in every entry\n",
        "  # https://pytorch.org/docs/stable/tensors.html\n",
        "  return torch.FloatTensor(embeddings), vocab\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0g9s_WVKM2l"
      },
      "source": [
        "This format_data() function is where you will add code to determine each word's position from m1 and m2.  As a reminder, we don't want to have negative values in m1_pos_list or m2_pos_list.  To address this, negative values will begin indexing after max_length (300).  For example, the position -10 would be stored as 310, the position -17 would be stored at 317, and so on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8m1yjWIR8v"
      },
      "source": [
        "import csv\n",
        "def format_data(filename, vocab, labels, max_length):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      filename: pointer to file holding the dataset we wish to process\n",
        "      vocab: GLoVE vocabulary file created from read_embeddings function\n",
        "      labels: dictionary mapping relationship name to integer index\n",
        "      max_length: maximum length of input\n",
        "    Returns:\n",
        "      data: Input sentences processed as glove embedding indices\n",
        "      data_m1: For each example in the dataset there is a list of positions (one for each word)\n",
        "                from the word to the first entity (appended with _ent1_) with no negative values\n",
        "      data_m2: For each example in the dataset there is a list of positions (one for each word)\n",
        "                from the word to the second entity (appended with _ent2_) with no negative values\n",
        "      data_labels:  Includes the integer label associated with each example in the dataset\n",
        "    \"\"\"    \n",
        "    data = []\n",
        "    data_labels = []\n",
        "    data_m1 = []\n",
        "    data_m2 = []\n",
        "    file = open(filename)\n",
        "    csvreader = csv.reader(file, delimiter=',')\n",
        "\n",
        "    for line in csvreader:\n",
        "        sentence = line[0]\n",
        "        label = line[1]\n",
        "        \n",
        "        m1_pos_list = []\n",
        "        m2_pos_list = []\n",
        "        split_sentence = sentence.split(\" \")\n",
        "\n",
        "        #YOUR CODE HERE\n",
        "        for idx, word in enumerate(split_sentence):\n",
        "          # find index of entity 1 in sentence\n",
        "          if word.find(\"_ent1_\"):\n",
        "            ent_1_idx = idx\n",
        "          # find index of entity 2 in sentence\n",
        "          if word.find(\"_ent2_\"):\n",
        "            ent_2_idx = idx\n",
        "\n",
        "        \n",
        "        for idx, word in enumerate(split_sentence):\n",
        "          # calculate each word's distance to entity 1\n",
        "          if idx >= ent_1_idx:\n",
        "            m1_pos_list.append(idx - ent_1_idx)\n",
        "          if idx < ent_1_idx:\n",
        "            m1_pos_list.append(300 + ent_1_idx - idx)\n",
        "\n",
        "          # calculate each word's distance to entity 2\n",
        "          if idx >= ent_2_idx:\n",
        "            m2_pos_list.append(idx - ent_2_idx)\n",
        "          if idx < ent_2_idx:\n",
        "            m2_pos_list.append(300 + ent_2_idx - idx)\n",
        "        #END YOUR CODE HERE\n",
        "\n",
        "        w_int = []\n",
        "        for w in nltk.word_tokenize(sentence.lower()):\n",
        "            # skip the unknown words\n",
        "            if w in vocab:\n",
        "                w_int.append(vocab[w])\n",
        "            else:\n",
        "                w_int.append(UNKNOWN_INDEX)\n",
        "        data_lens.append(len(w_int))\n",
        "\n",
        "        #makes sure the example isn't too long for our model\n",
        "        if len(w_int) < 300:\n",
        "          w_int.extend([PAD_INDEX] * (max_length - len(w_int)))\n",
        "          data.append((w_int))\n",
        "          m1_pos_list.extend([max_length-1] * (max_length-len(m1_pos_list)))\n",
        "          data_m1.append(m1_pos_list)\n",
        "          m2_pos_list.extend([max_length*2-1] * (max_length-len(m2_pos_list)))\n",
        "          data_m2.append(m2_pos_list)\n",
        "          data_labels.append(labels[label])\n",
        "    return data, data_m1, data_m2, data_labels"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1raXXcehISDr"
      },
      "source": [
        "class EntityCNNClassifier(nn.Module):\n",
        "\n",
        "   def __init__(self, params, pretrained_embeddings):\n",
        "      super().__init__()\n",
        "      self.seq_len = params[\"max_seq_len\"]\n",
        "      self.num_labels = params[\"label_length\"]\n",
        "      self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
        "\n",
        "      #YOUR CODE HERE\n",
        "      self.m1_embeddings = nn.Embedding(num_embeddings=600, embedding_dim=16, padding_idx=PAD_INDEX)\n",
        "      self.m2_embeddings = nn.Embedding(num_embeddings=600, embedding_dim=16, padding_idx=PAD_INDEX)\n",
        "      #END YOUR CODE HERE\n",
        "\n",
        "      self.conv_2 = nn.Conv1d(82, 16, 2, 1) #in_channels, out_channels, kernel_size, stride\n",
        "      self.pool_2 = nn.MaxPool1d(299,1) #kernel_size, stride\n",
        "\n",
        "      self.fc = nn.Linear(16, self.num_labels)\n",
        "    \n",
        "   def forward(self, input, m1_pos_list, m2_pos_list): \n",
        "      x_word_emb = self.embeddings(input)\n",
        "      \n",
        "      #YOUR CODE HERE\n",
        "      x_m1 = self.m1_embeddings(m1_pos_list)\n",
        "      x_m2 = self.m2_embeddings(m2_pos_list)\n",
        "      #END YOUR CODE HERE\n",
        "\n",
        "      x = torch.cat((x_word_emb, x_m1, x_m2), 2)\n",
        "      x = x.permute(0, 2, 1)\n",
        "    \n",
        "      conv = self.conv_2(x)\n",
        "      conv = torch.tanh(conv)\n",
        "      conv = self.pool_2(conv)\n",
        "      conv = conv.view((conv.shape[0], -1))\n",
        "\n",
        "\n",
        "      self.out = self.fc(conv)\n",
        "      return self.out.squeeze()\n",
        "\n",
        "   def evaluate(self, x, s1, s2, y):\n",
        "      \n",
        "      self.eval()\n",
        "      corr = 0.\n",
        "      total = 0.\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for x, s1, s2, y in zip(x,s1, s2, y):\n",
        "          y_preds=self.forward(x, s1, s2)\n",
        "          for idx, y_pred in enumerate(y_preds):\n",
        "              prediction=torch.argmax(y_pred)\n",
        "              if prediction == y[idx]:\n",
        "                corr += 1.\n",
        "              total+=1                          \n",
        "      return corr/total\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKM9sDbNkQoj"
      },
      "source": [
        "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
        "cnn_train_x, cnn_train_s1, cnn_train_s2, cnn_train_y = format_data(train_dataset, cnn_vocab, labels, 300)\n",
        "cnn_dev_x, cnn_dev_s1, cnn_dev_s2, cnn_dev_y = format_data(dev_dataset, cnn_vocab, labels, 300)\n",
        "cnn_trainX, cnn_trainS1, cnn_trainS2, cnn_trainY=get_batches(cnn_train_x, cnn_train_s1, cnn_train_s2, cnn_train_y, torch.LongTensor)\n",
        "cnn_devX, cnn_devS1, cnn_devS2, cnn_devY=get_batches(cnn_dev_x, cnn_dev_s1, cnn_dev_s2, cnn_dev_y, torch.LongTensor)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUzgVW-DTEyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4ad3a0-04b5-4788-8771-2821c472c43c"
      },
      "source": [
        "cnnmodel = EntityCNNClassifier(params={\"max_seq_len\": 100, \"label_length\": len(labels)}, pretrained_embeddings=embs)\n",
        "\n",
        "optimizer = torch.optim.Adam(cnnmodel.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "losses = []\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=15\n",
        "best_dev_acc = 0.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    cnnmodel.train()\n",
        "\n",
        "    for x, s1, s2, y in zip(cnn_trainX, cnn_trainS1, cnn_trainS2, cnn_trainY):\n",
        "      y_pred = cnnmodel.forward(x, s1, s2)\n",
        "      loss = cross_entropy(y_pred.view(-1, cnnmodel.num_labels), y.view(-1))\n",
        "      losses.append(loss) \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    dev_accuracy=cnnmodel.evaluate(cnn_devX, cnn_devS1, cnn_devS2, cnn_devY)\n",
        "    if epoch % 1 == 0:\n",
        "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "        if dev_accuracy > best_dev_acc:\n",
        "          torch.save(cnnmodel.state_dict(), 'best-cnnmodel-parameters.pt')\n",
        "          best_dev_acc = dev_accuracy\n",
        "\n",
        "cnnmodel.load_state_dict(torch.load('best-cnnmodel-parameters.pt'))\n",
        "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, dev accuracy: 0.387\n",
            "Epoch 1, dev accuracy: 0.464\n",
            "Epoch 2, dev accuracy: 0.479\n",
            "Epoch 3, dev accuracy: 0.541\n",
            "Epoch 4, dev accuracy: 0.545\n",
            "Epoch 5, dev accuracy: 0.564\n",
            "Epoch 6, dev accuracy: 0.573\n",
            "Epoch 7, dev accuracy: 0.600\n",
            "Epoch 8, dev accuracy: 0.603\n",
            "Epoch 9, dev accuracy: 0.620\n",
            "Epoch 10, dev accuracy: 0.626\n",
            "Epoch 11, dev accuracy: 0.632\n",
            "Epoch 12, dev accuracy: 0.628\n",
            "Epoch 13, dev accuracy: 0.632\n",
            "Epoch 14, dev accuracy: 0.630\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MviKzbYqXqTg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}